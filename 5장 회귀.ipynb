{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 회귀 소개\n",
    "\n",
    "회귀(Regression)는 현대 통계학을 떠받치고 있는 주요 기둥 중 하나입니다. 회귀 기반의 분석은 엔지니어링, 의학, 사회과학, 경제학등 분야가 발전하는 데 크게 기여해왔습니다. 갈콘(Galton)의 키에대한 유전 연구를 통해 __사람의 키는 평균 키로 회귀 하려는 경향을 가진다는 자연의 법칙이 있다는 것입니다.__  \n",
    "\n",
    "통계학 용어를 빌리자면 회귀는 여러 개의 독립변수와 한 개의 종속변수 간의 상관관계를 모델링하는 기법을 통칭합니다.  \n",
    "Y = X1W1 + X2W2 +...XnWn, Y는 종속변수, X는 독립변수, W는 독립변수의 값에 영향을 미치는 회귀계수(Regression coefficients)입니다. 머신러닝 관점에서 독립변수는 피처에 해당되며 종속변수는 결정 값입니다. 머신러닝 회귀 예측의 핵심은 주어진 피처와 결정 값 데이터 기반에서 학습을 통해 __최적의 회귀 계수__를 찾아내는 것입니다.\n",
    "\n",
    "회귀는 회귀 계수의 선형/비선형 여부, 독립변수의 개수, 종속변수의 개수에 따라 여러 가지 유형으로 나눌 수 있습니다.\n",
    "\n",
    "지도 학습은 두 가지유형으로 나뉘는데, 바로 분류와 회귀입니다. 이 두가지 기법의 가장 큰 차이는 분류는 예측값이 카테고리와 같은 이산형 클래스 값이고 회귀는 연속형 숫자 갑이라는 것입니다. 여러 가지 회귀 중에서 선형 회귀가 가장 많이 사용됩니다. 선형 회귀는 실제 값과 예측값의 차이(오류의 제곱값)를 최소화 하는 직선형 회귀선을 최적화 하는 방식입니다.\n",
    "\n",
    " + 일반 선형 회귀: 예측값과 실제값의 RSS(Residual Sum of Squares)를 최소화할 수 있도록 회귀 계수를 최적화하며, 규제(Regularization)를 적용하지 않은 모델입니다.\n",
    " + 릿지(Ridge): 릿지 회귀는 선형 회귀에 L2 규제를 추가한 회귀 모델입니다. 릿지 회귀는 L2 규제를 적용하는데, L2 규제는 상대적으로 큰 회귀 계수 값의 예측 영향도를 감소시키기 위해서 회귀 계수값을 더 작게 만드는 규제 모델입니다.\n",
    " + 라쏘(Lasso): 라쏘 회귀는 선형 회귀에 L1 규제를 적용한 방식입니다. L2 규제가 회귀 계수 값의 크리를 줄이는 데 반해, L1 규제는 예측 영량력이 작은 피처의 회귀 계수를 0으로 만들어 회귀 예측 시 피처가 선택되지 않게 하는 것입니다. 이러한 특성 때문에 L1 규제는 피처 선택 기능으로도 불립니다.\n",
    " + 엘라스틱넷(ElasticNet): L2,L1 규제를 함께 결합한 모델입니다. 주로 피처가 많은 데이터 세트에서 적용되며, L1규제로 피처의 개수를 줄임과 동시에 L2 규제로 계수 값의 크기를 조정합니다.\n",
    " + 로지스틱 회귀(Logistic Regression): 로지스틱 회귀는 회귀라는 이름이 붙어 있지만, 사실은 분류에 사용되는 선형 모델입니다. 로지스틱 회귀는 매우 강력한 분류 알고리즘입니다. 일반적으로 이진 분류뿐만 아니라 희소 영역의 분류, 예를 들어 텍스트 분류와 같은 영역에서 뛰러나 예측 성능을 보입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 단순 선형 회귀를 통한 회귀 이해\n",
    "\n",
    "단순 선형 회귀는 독립변수도 하나, 종속변수도 하나인 선형 회귀입니다. 실제 값과 회귀 모델의 차이에 따른 오류 값을 남은 오류, 즉 잔차라고 부릅니다. 최적의 회귀 모델을 만든다는 것은 바로 전체 데이터의 잔차(오류 값) 합이 최고사 되는 모델을 만든다는 의미입니다. 동시에 오류 값 합이  최소가 될 수 있는 최적의 회귀 계수를 찾는다는 의미도 됩니다.\n",
    "\n",
    "오류 값은 +,- 가 될 수 있는데 전체 데이터의 오류 합을 구하기 위해 단순히 더했다가는 뜻하지 않게 오류 합이 크게 줄어들 수 있습니다. 따라서 보통 오류 합을 계산할 때는 절댓값을 취해서 더하거나(Mean Absolute Error), 오류 값의 제곱을 구해서 다하느 방식(Residual Sum of Square)을 취합니다. 일반적으로 미분 등의 계산을 편리하게 하기 위해서 RSS 방식으로 오류 합을 구합니다. 즉, Error^2 = RSS 입니다.\n",
    "\n",
    "회귀에서 RSS는 비용(Cost)이며 W변수(회귀 계수)로 구성되는 RSS를 비용함수라고 합니다. 머신러닝 회귀 알고리즘은 데이터를 계속 학습하면서 이 비용 함수가 반환하는 값(즉, 오류 값)을 지속해소 감소시키고 최종적으로는 더 이상 감소하지 않는 최소의 오류 값을 구하는 것입니다. 비용 함수를 손실 함수(loss function)이라고도 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 비용 최소화 하기 - 경사 하강법(Gradient Descent) 소개\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
