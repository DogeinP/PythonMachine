{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 분류(Classification)의 개요\n",
    "\n",
    "    지도학습 (Label), 즉 명시적인 정답이 있는 데이터가 주어진 상태에서 학습하는 머신러닝 방식입니다. 지도학습의 대표적인 유형인 분류(Classification)는 학습 데이터로 주어진 데이터의 피처와 레이블값(결정 값, 클래스 값)을 머신러닝 알고리즘으로 학습해 모델을 생성하고, 이렇게 생성된 모델에 새로운 데이터 값이 주어졌을 때 미지의 레이블 값을 예측하는 것입니다.\n",
    "    \n",
    "    - 베이즈(Bayes) 통계와 생성 모델이 기반한 나이브 베이즈(Naiive Bayes)\n",
    "    - 독립변수와 종속변수의 선형 관셰성에 기반한 로지스틱 회귀(Logistic Regression)\n",
    "    - 데이터 균일도에 따른 규칙 기반의 결정 트리(Decision Tree)\n",
    "    - 개별 클래스 간의 최대 분류 마진을 효과적으로 찾아주는 서포트 벡터 머신(Support Vector Machine)\n",
    "    - 근접 거리를 기준으로 하는 최소 근접(Nearest Neighbor) 알고리즘\n",
    "    - 심층 연결 기반의 신경망(Nerural Network)\n",
    "    - 서로 다른(또는 같은) 머신러닝 알고리즘을 결합한 앙상블(Ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    앙상블은 서로 다른/또는 같은 알고리즘을 단순히 결합한 형태도 있으나, 일반적으로 배깅(Bagging)과 부스팅(Boosting) 방식으로 나뉩니다. 배깅 방식의 대표인 랜덤 포레스트(Random Forest)는 뛰어난 예측 성능, 상대적으로 빠른 수행 시간, 유연승 등으로 많은 분석가가 애용하는 알고리즘 입니다. 하지만 근래의 앙상블 방법은 부스팅 방식으로 지속해서 발전하고 있습니다. 부스팅의 효시라고 할 수 있는 그래디언트 부스팅(Gradient Boosting)의 경우 뛰어난 예측 성능을 가지고 있지만, 수행 시간이 너무 오래 걸리는 단점으로 인해 최적화 모델 튜닝이 어려웠습니다. 하지만 XgBoost(eXtra Gradient Boost)와 LightGBM 등 기존 그래디언트 부스팅의 예측 성능을 한 단계 발전시키면서도 수행 시간능 단축시킨 알고리즘이 계속 등장하면서 정형 데잍의 분류 영역에서 가장 활용도가 높은 알고리즘으로 자리 잡았습니다.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 결정트리\n",
    "    \n",
    "    매우 쉽고 유연하게 적용될 수 있는 알고리즘입니다. 또한 데이터의 스케일링이나 정규화등의 사전 가공이 영향이 적습니다. 하지만 예측 성능을 향상시키기 위해 복잡한 규치직 구조를 가져야 하며, 이로 인한 과적합(Overfitting)이 발생해 반대로 예측성능이 저하될 수도 있다는 단점이 있습니다. 하지만 이러한 단점이 앙상블 기법에서는 오히려 장점으로 작용합니다. 앙상블은 매우 많은 여러개의 약한 학습기(즉, 예측 성능이 상대적을 떨어지는 학습 알고리즘)를 결합해 확룔적 보완과 오류가 발생한 부분에 대한 가중치를 계속 업데이트하면서 예측 성능을 향상시키는데, 결정 트리가 좋은 약한 학습기가 되기 때문입니다.\n",
    "    \n",
    "    직관적으로 이해하기 쉬운 알고리즘 입니다. 데이터의 어떤 기준을 바탕으로 규칙을 만들어야 가장 효율적인 분류가 될 것인가가 알고리즘의 성능을 크게 좌우합니다.\n",
    "    규칙 노드(Decision Node)로 표시된 노드는 규치 조건이 되는 것이고, 리프 노드(Leaf Node)로 표시된 노드는 결정된 클래스 값입니다. 그렇게 새로운 서브 노트(Sub Node)가 생성됩니다. 많은 규칙이 있다는 것은 곧 분류를 결정하는 방식이 더욱 복잡해즌다는 얘기이고, 이는 곧 과적합으로 이어지기 쉽습니다. 즉, 트리의 깊이(Depth)가 깊어질수록 결정트리의 예측 성능이 저하될 가능성이 높습니다.\n",
    "    가능한 한 적은 결정 노드로 높은 예측 정확도를 가지려면 데이터를 분류할 때 최대한 많은 데이터 세트가 해당 분류에 속할 수 있도록 결정 노드 구칙이 정해져야 합니다. 최대한 균일한 데이터 세트를 구성할 수 있도록 분할 하는 것이 필요합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    균일한 데이터 세트란, 데이터를 구분하는 데 필요한 정보의 양에 영향을 미치는 정도가 적은 데이터로 균일도가 낮은 데이터는 같은 조건에서 데이터를 판단하는데 있어 더 많은 정보가 필요합니다.\n",
    "    결정 노드는 정보 균일도가 높은 데이터 세트를 먼저 선택할 수있도록 규칙 조건을 만듭니다. 즉, 정보 균일도가 데이터 세트로 쪼개질 수 있도록 조건을 찾아 서브 데이터 세트를 만들고, 다시 이 서브 데이터ㅓ 세트에서 균일도가 높은 자식 데이터 세트로 쪼개는 방식을 자식 트리로 내려가면서 반복하는 방식으로 데이터 값으 예측하게 됩니다.\n",
    "    이러한 정보의 균일도를 측정하는 대표적인 방법은 엔트로피를 이용한 정보 이득(Information Gain)지수와 지니(Gini)계수가 있습니다.\n",
    "    \n",
    "    정보이득은 엔트로피 개념을 기반으로 하며 엔트로피는 주어진 데이터 집합의 혼잡도를 의미하는데, 서로 다른 값이 섞여 있으면 엔트로피가 높고, 같은 값이 섞여 있으면 엔트로프가 낮습니다. 정보 이득 지수는 1에서 엔트로피 지수를 뺀 값입니다. 1- 엔트로프 지수. 결정트리는 이 정보 이득 지수로 분할 기준을 정합니다. 즉, 정보이득이 높은 속성을 기준으로 분할합니다.\n",
    "    지니 계수는 원래 경제학에서 불평등 지수를 나타낼 때 사용하는 계수입니다. 0이 가장 평등하고 1로 갈수록 불평등합니다. 지니 계수가 낮을수록 데이터 균일도가 높은 것으로 해석해 지니 계수가 낮은 속성을 기준으로 분할합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
